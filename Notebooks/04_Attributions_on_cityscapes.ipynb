{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a7ee11",
   "metadata": {},
   "source": [
    "## Notebook Title: Attributions on Cityscapes\n",
    "\n",
    "This notebook demonstrates how SegClarity can be used to compute, visualize, and interpret attributions for deep learning models trained on the Cityscapes semantic segmentation dataset. It leverages XAI techniques to better understand model predictions at the pixel and region level for urban scene understanding.\n",
    "#\n",
    "**Purpose**:  \n",
    "- To apply attribution methods (e.g., Grad-CAM, DeepLift, LRP) to segmentation models trained on Cityscapes images.\n",
    "- To visualize and compare attribution maps with original street scene images and ground truth masks.\n",
    "- To facilitate model interpretability and support error analysis in semantic segmentation for urban environments.\n",
    "#\n",
    "**Key Features**:\n",
    "- Loads pre-trained segmentation model (UNET) for the Cityscapes dataset.\n",
    "- Supports multiple attribution methods from Captum and custom utilities.\n",
    "- Generates and visualizes attribution heatmaps over Cityscapes images.\n",
    "- Provides side-by-side comparison of attributions, input images, and segmentation masks.\n",
    "#\n",
    "**Workflow**:\n",
    "1. Configure dataset and model parameters for Cityscapes.\n",
    "2. Load the appropriate pre-trained segmentation model.\n",
    "3. Select and apply attribution methods to validation images.\n",
    "4. Visualize attribution maps alongside original images and ground truth.\n",
    "5. Analyze and interpret the results for model transparency and insight into model behavior.\n",
    "#\n",
    "**Dataset**:  \n",
    "- Cityscapes: 20-class semantic segmentation of urban street scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8de6db1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found or incomplete. Please make sure all required folders for the specified \"split\" and \"mode\" are inside the \"root\" directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m absolute_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     39\u001b[0m dataset_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcityscapes\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 41\u001b[0m _, val_loader, _  \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_cityscapes_dataloaders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mabsolute_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcityscapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43maugment_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m base_dir: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../datasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/app/Notebooks/../Modules/CityscapeDataset/core.py:73\u001b[0m, in \u001b[0;36mcreate_cityscapes_dataloaders\u001b[0;34m(root, batch_size, num_workers, image_size, augment_train)\u001b[0m\n\u001b[1;32m     70\u001b[0m test_transform \u001b[38;5;241m=\u001b[39m get_transforms(image_size, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Create datasets\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCityscapesDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msemantic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_size\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m CityscapesDataset(\n\u001b[1;32m     83\u001b[0m     root\u001b[38;5;241m=\u001b[39mroot, \n\u001b[1;32m     84\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m     image_size\u001b[38;5;241m=\u001b[39mimage_size\n\u001b[1;32m     89\u001b[0m )\n\u001b[1;32m     91\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CityscapesDataset(\n\u001b[1;32m     92\u001b[0m     root\u001b[38;5;241m=\u001b[39mroot, \n\u001b[1;32m     93\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m     image_size\u001b[38;5;241m=\u001b[39mimage_size\n\u001b[1;32m     98\u001b[0m )\n",
      "File \u001b[0;32m/app/Notebooks/../Modules/CityscapeDataset/core.py:17\u001b[0m, in \u001b[0;36mCityscapesDataset.__init__\u001b[0;34m(self, root, split, mode, target_type, transforms, image_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfine\u001b[39m\u001b[38;5;124m'\u001b[39m, target_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msemantic\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     16\u001b[0m              transforms\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, image_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m512\u001b[39m)):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;241m=\u001b[39m transforms\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size \u001b[38;5;241m=\u001b[39m image_size\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/cityscapes.py:156\u001b[0m, in \u001b[0;36mCityscapes.__init__\u001b[0;34m(self, root, split, mode, target_type, transform, target_transform, transforms)\u001b[0m\n\u001b[1;32m    154\u001b[0m         extract_archive(from_path\u001b[38;5;241m=\u001b[39mtarget_dir_zip, to_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    157\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or incomplete. Please make sure all required folders for the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m specified \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m are inside the \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m directory\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    159\u001b[0m         )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m city \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages_dir):\n\u001b[1;32m    162\u001b[0m     img_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages_dir, city)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dataset not found or incomplete. Please make sure all required folders for the specified \"split\" and \"mode\" are inside the \"root\" directory"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# Add parent directory to path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Modules.Architecture import generate_model\n",
    "from Modules.ModelXAI import generate_XAI_model\n",
    "from Modules.Utils import (\n",
    "    get_layer_by_name,\n",
    "    clip_fixed_percentage,\n",
    "    Normalizations,\n",
    "    apply_otsu,\n",
    ")\n",
    "import Modules.Evaluation as ev\n",
    "\n",
    "from Modules.Attribution.constants import (\n",
    "    LUNET_LAYERS_JOURNAL,\n",
    "    UNET_LAYERS_JOURNAL,\n",
    ")\n",
    "from Modules.Attribution import generateAttributions\n",
    "\n",
    "from Modules.Visualization import (\n",
    "    generate_heatmap,\n",
    ")\n",
    "from Modules.Visualization.tools import split_components\n",
    "from Modules.Visualization.core import visualize_image\n",
    "\n",
    "from Modules.CityscapeDataset.core import create_cityscapes_dataloaders\n",
    "from Modules.CityscapeDataset.tools import decode_segmap\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "absolute_path = Path('..')\n",
    "\n",
    "dataset_type='cityscapes'\n",
    "\n",
    "_, val_loader, _  = create_cityscapes_dataloaders(\n",
    "    root=absolute_path / \"datasets\" / \"cityscapes\",\n",
    "    batch_size=1,\n",
    "    num_workers=2,\n",
    "    augment_train=None,\n",
    ")\n",
    "\n",
    "\n",
    "base_dir: str = f\"../datasets/{dataset_type}\"\n",
    "device = 'cuda:0'\n",
    "model_type = 'unet'\n",
    "\n",
    "OUT_CHANNELS = 20\n",
    "models_path = f\"../models/{dataset_type}/{model_type}\"\n",
    "model_name = f\"best_model.pth\"\n",
    "\n",
    "\n",
    "print(f\"Loading model from {models_path}\")\n",
    "print(f\"Model name: {model_name}\")\n",
    "\n",
    "\n",
    "# %%\n",
    "print(\"Loading model...\")\n",
    "model = generate_model(\n",
    "    model_type= model_type, \n",
    "    out_channels=OUT_CHANNELS, \n",
    "    load_from_checkpoint=True,\n",
    "    models_path=models_path, \n",
    "    checkpoint_name=model_name\n",
    ").eval().to(device)\n",
    "\n",
    "# Prepare model for XAI\n",
    "model = generate_XAI_model(model=model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ee9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch,mask_batch,names in val_loader:\n",
    "    image_batch = image_batch.to(device)\n",
    "    mask_batch = mask_batch.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73cf62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred= model(image_batch).max(dim=1)[1]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(image_batch[0].permute(1, 2, 0).detach().cpu().numpy() * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]));_ = ax[0].axis('off'); ax[0].set_title('Image')\n",
    "ax[1].imshow(decode_segmap(mask_batch[0].detach().cpu()));_ = ax[1].axis('off'); ax[1].set_title('Mask')\n",
    "fig.tight_layout()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f78216a",
   "metadata": {},
   "source": [
    "### Attribution Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose method between LayerLRP, LayerDeepLift, LayerGradientXActivation, LayerGradCam\n",
    "method = \"LayerDeepLift\"\n",
    "selected_target = 14\n",
    "\n",
    "layers = LUNET_LAYERS_JOURNAL if model_type == \"lunet\" else UNET_LAYERS_JOURNAL\n",
    "layers = list(layers.keys())\n",
    "layer_name = layers[-3];print(layer_name)\n",
    "\n",
    "layer_mapper = {\n",
    "    \"conv1.0\": \"Dec4\",\n",
    "    \"conv2.0\": \"Dec3\",\n",
    "    \"conv3.0\": \"Dec2\",\n",
    "    \"conv4.0\": \"Dec1\",    \n",
    "    \"conv5.0\": \"Dec4\",\n",
    "    \"conv6.0\": \"Dec3\",\n",
    "    \"conv7.0\": \"Dec2\",\n",
    "    \"conv8.0\": \"Dec1\",\n",
    "    \"final_layer\": \"FL\",\n",
    "}\n",
    "\n",
    "attr = generateAttributions(image_batch, model, selected_target, method, get_layer_by_name(model, layer_name))\n",
    "attr = clip_fixed_percentage(attr,p=0.05)\n",
    "\n",
    "if attr.max() > 10e4:\n",
    "    attr = Normalizations.pick('normalize_log')(attr)\n",
    "\n",
    "pv,nv,zv = split_components(attr,zero_threshold=0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989b7e3a",
   "metadata": {},
   "source": [
    "### Visualize heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,3,figsize=(16,8))\n",
    "fig.suptitle(f\"{method} on {layer_mapper[layer_name]}\",y=0.75)\n",
    "\n",
    "\n",
    "saliency_mask = pv - nv\n",
    "if nv.sum() == 0:\n",
    "    cmap=\"YlGn\"\n",
    "elif pv.sum() == 0:\n",
    "    cmap=\"YlOrRd\"\n",
    "    saliency_mask = -saliency_mask\n",
    "else:\n",
    "    cmap=\"RdYlGn\"\n",
    "viz_image = image_batch.squeeze().detach().cpu()\n",
    "\n",
    "viz_image = torch.from_numpy(resize(viz_image, (3,*attr.shape[-2:]), anti_aliasing=True))\n",
    "\n",
    "print(viz_image.shape)\n",
    "\n",
    "_ = visualize_image(viz_image,fig=fig,ax=axes[0]);axes[0].set_title(\"Saliency Overlay\")\n",
    "axes[0].imshow(pv.detach().cpu().squeeze().numpy(), cmap=cmap, alpha=0.5); axes[0].axis(\"off\")\n",
    "            \n",
    "_ = generate_heatmap(pv, fig=fig, ax=axes[1],title=\"Positive\")\n",
    "_ = generate_heatmap(nv, fig=fig, ax=axes[2],title=\"Negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f9e047",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0e0f56",
   "metadata": {},
   "source": [
    "**PG Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56248fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_metric = ev.PointingGameMetric()\n",
    "\n",
    "M = torch.zeros_like(mask_batch)\n",
    "M[mask_batch == selected_target] = 1\n",
    "pg_score = pg_metric.calculate(saliency_map=attr,gt_mask=M.squeeze())\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(M[0].detach().cpu().numpy(),cmap='inferno');plt.axis('off')\n",
    "\n",
    "from captum.attr import LayerAttribution\n",
    "saliency_map = attr.detach().cpu().clone()\n",
    "if saliency_map.shape[-2:] != mask_batch.shape[-2:]:\n",
    "    saliency_map = LayerAttribution.interpolate(\n",
    "                    saliency_map, mask_batch.squeeze().shape[-2:]\n",
    "    )\n",
    "\n",
    "p = torch.argmax(saliency_map).item()\n",
    "h,w = saliency_map.shape[-2:]\n",
    "l = p // w\n",
    "c = p % w\n",
    "\n",
    "plt.scatter(c,l,color='red',s=50,marker='x')\n",
    "msg = \"Hit\" if M[0,l,c] == 1 else \"Miss\"\n",
    "\n",
    "_= plt.title(f\"{msg} at ({l},{c})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e27e7a",
   "metadata": {},
   "source": [
    "**CH Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d312be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_metric = ev.ContentHeatmapMetric()\n",
    "\n",
    "ch_score = ch_metric.calculate(attr=attr,mask=mask_batch,target=selected_target)\n",
    "print(ch_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9376f2",
   "metadata": {},
   "source": [
    "**ACS Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dd3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_metric = ev.ACSMetric()\n",
    "\n",
    "metrics, _ = acs_metric.calculate(attr,mask_batch.detach().cpu(), selected_target)\n",
    "acs = metrics[\"f1\"]\n",
    "attr_otsu, _ = apply_otsu(attr.detach().cpu())\n",
    "metrics_otsu, _ = acs_metric.calculate(attr_otsu,mask_batch.detach().cpu(),selected_target)\n",
    "acs_otsu = metrics_otsu[\"f1\"]\n",
    "\n",
    "fig,axes = plt.subplots(1,2,figsize=(12,6))\n",
    "generate_heatmap(attr,fig=fig,ax=axes[0],title=f'Score: {acs:.3f} [without Otsu]')\n",
    "generate_heatmap(attr_otsu,fig=fig,ax=axes[1],title=f'Score: {acs_otsu:.3f} [with Otsu]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac0b11",
   "metadata": {},
   "source": [
    "**Sensitivity max**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28218b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_metric = ev.SensitivityMaxMetric(n_perturb_samples=10,perturb_radius=0.02,max_examples_per_batch=1)\n",
    "sens_score = sens_metric.calculate(image_batch,model,method,get_layer_by_name(model, layer_name),selected_target).item()\n",
    "print(sens_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a12bfd0",
   "metadata": {},
   "source": [
    "**Infidelity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee93e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.Evaluation import generate_random_squares\n",
    "\n",
    "def square_removal_perturbation(image,mask):\n",
    "    image = image.clone()\n",
    "    mask = mask.clone()\n",
    "    image[:,mask==1] = 0\n",
    "    return image,mask\n",
    "\n",
    "squares = generate_random_squares(*mask_batch.shape[-2:],25,30)\n",
    "\n",
    "M = torch.zeros_like(mask_batch)\n",
    "for square in squares:\n",
    "    M[:,square.y:square.y+square.size,square.x:square.x+square.size] = 1\n",
    "\n",
    "targeted_mask = torch.where(mask_batch == selected_target, torch.ones_like(mask_batch), torch.zeros_like(mask_batch))\n",
    "\n",
    "fig,axes = plt.subplots(2,3,figsize=(15,6))\n",
    "axes[0,0].imshow((image_batch)[0].permute(1, 2, 0).detach().cpu().numpy() * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]));axes[0,0].axis('off'); axes[0,0].set_title('Image')\n",
    "axes[0,1].imshow(decode_segmap(mask_batch.squeeze().detach().cpu()));axes[0,1].axis('off');axes[0,1].set_title('GT')\n",
    "axes[0,2].imshow(decode_segmap((targeted_mask * selected_target).squeeze().detach().cpu()));axes[0,2].axis('off');axes[0,2].set_title('Targeted Mask')\n",
    "axes[1,0].imshow(M[0].detach().cpu().numpy(),cmap='gray');axes[1,0].axis('off');axes[1,0].set_title('Generated Mask')\n",
    "axes[1,1].imshow(decode_segmap((mask_batch*(1-M)).squeeze().detach().cpu()));axes[1,1].axis('off');axes[1,1].set_title('Masked GT')\n",
    "axes[1,2].imshow(decode_segmap((mask_batch * (1 - (targeted_mask) * M) ).squeeze().detach().cpu()));axes[1,2].axis('off');axes[1,2].set_title('Targeted Masked GT')\n",
    "fig.tight_layout()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b250d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "\n",
    "infidelity_metric = ev.InfidelityMetric()\n",
    "infidelity_score = infidelity_metric.calculate(model, image_batch, mask_batch, selected_target, attr, n_samples, n_squares=50, size=10)\n",
    "print(infidelity_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
