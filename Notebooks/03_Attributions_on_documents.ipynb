{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a7ee11",
   "metadata": {},
   "source": [
    "# Notebook Title: Attributions on Documents\n",
    "\n",
    "\n",
    "This notebook demonstrates how SegClarity work to compute, visualize, and interpret attributions for deep learning models trained on document segmentation datasets. It leverages XAI techniques to better understand model predictions at the pixel and region level.\n",
    "\n",
    "**Purpose**:  \n",
    "- To apply attribution methods (e.g., Grad-CAM, DeepLift, LRP) to segmentation models trained on document images.\n",
    "- To visualize and compare attribution maps with original images and ground truth masks.\n",
    "- To facilitate model interpretability and support error analysis in document segmentation.\n",
    "\n",
    "**Key Features**:\n",
    "- Loads pre-trained segmentation models (LUNet, UNet) for different document datasets (UTP, splitAB1).\n",
    "- Supports multiple attribution methods from Captum and custom utilities.\n",
    "- Generates and visualizes attribution heatmaps over document images.\n",
    "- Provides side-by-side comparison of attributions, input images, and segmentation masks.\n",
    "\n",
    "**Workflow**:\n",
    "1. Configure dataset and model parameters.\n",
    "2. Load the appropriate pre-trained segmentation model.\n",
    "3. Select and apply attribution methods to test images.\n",
    "4. Visualize attribution maps alongside original images and ground truth.\n",
    "5. Analyze and interpret the results for model transparency.\n",
    "\n",
    "**Datasets**:  \n",
    "- UTP: 7-class document segmentation.\n",
    "- splitAB1: 4-class document segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8de6db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "from sys import path\n",
    "path.append('..')\n",
    "from Modules.Dataset import generate_dataset\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Modules.Dataset import generate_dataset\n",
    "from Modules.Architecture import generate_model\n",
    "from Modules.ModelXAI import generate_XAI_model\n",
    "\n",
    "from Modules.Utils import (\n",
    "    get_layer_by_name,\n",
    "    clip_fixed_percentage,\n",
    "    Normalizations,\n",
    ")\n",
    "\n",
    "from Modules.Attribution.constants import (\n",
    "    LUNET_LAYERS_JOURNAL,\n",
    "    UNET_LAYERS_JOURNAL,\n",
    ")\n",
    "from Modules.Attribution import generateAttributions\n",
    "\n",
    "from Modules.Visualization import (\n",
    "    visualize_mask,\n",
    "    visualize_image_and_mask,\n",
    "    generate_heatmap,\n",
    ")\n",
    "from Modules.Visualization.tools import split_components\n",
    "from Modules.Visualization.core import visualize_image\n",
    "\n",
    "# %%\n",
    "dataset_type='splitAB1'\n",
    "dataset_type='UTP'\n",
    "\n",
    "base_dir: str = f\"../datasets/{dataset_type}\"\n",
    "device = 'cpu'\n",
    "device = 'cuda:0'\n",
    "\n",
    "model_type = 'lunet'\n",
    "MODEL_TAGS = [\"from_scratch_models\", \"finetuned_models_minloss\"]\n",
    "model_tag = MODEL_TAGS[0] if dataset_type == 'UTP' else MODEL_TAGS[1]\n",
    "\n",
    "OUT_CHANNELS = 7 if dataset_type == 'UTP' else 4\n",
    "\n",
    "# Dataloader\n",
    "if dataset_type == 'splitAB1':\n",
    "    _, _, test_loader = generate_dataset(\n",
    "        dataset_type=dataset_type,\n",
    "        base_dir=base_dir,\n",
    "        **{\n",
    "            \"train_folder\": \"training-40\",\n",
    "            \"validation_folder\": \"validation\",\n",
    "            \"test_folder\": \"test\",\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    _, _, test_loader = generate_dataset(\n",
    "        dataset_type=dataset_type,\n",
    "        base_dir=base_dir,\n",
    "        **{\n",
    "            \"train_folder\": \"Training\",\n",
    "            \"validation_folder\": \"Validation\",\n",
    "            \"test_folder\": \"Test\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "UTP_EPOCHS_DICT = {\n",
    "    \"lunet\": {MODEL_TAGS[0]: 84, MODEL_TAGS[1]: 61},\n",
    "    \"unet\": {MODEL_TAGS[0]: 85, MODEL_TAGS[1]: 43},\n",
    "}\n",
    "\n",
    "\n",
    "CB55_EPOCHS_DICT = {\n",
    "    \"lunet\": {MODEL_TAGS[0]: 108, MODEL_TAGS[1]: 40},\n",
    "    \"unet\": {MODEL_TAGS[0]: 60, MODEL_TAGS[1]: 87},\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "if dataset_type == 'UTP':\n",
    "    models_path = f\"../models/{dataset_type}/{model_type}/{model_tag}\"\n",
    "    model_name = f\"model{UTP_EPOCHS_DICT[model_type][model_tag]}.pt\"\n",
    "else:\n",
    "    models_path = f\"../models/{dataset_type}/{model_type}/{model_tag}\"\n",
    "    model_name = f\"model{CB55_EPOCHS_DICT[model_type][model_tag]}.pt\"\n",
    "\n",
    "print(f\"Loading model from {models_path}\")\n",
    "print(f\"Model name: {model_name}\")\n",
    "\n",
    "\n",
    "# %%\n",
    "print(\"Loading model...\")\n",
    "model = generate_model(\n",
    "    model_type= model_type, \n",
    "    out_channels=OUT_CHANNELS, \n",
    "    load_from_checkpoint=True,\n",
    "    models_path=models_path, \n",
    "    checkpoint_name=model_name\n",
    ").eval().to(device)\n",
    "\n",
    "# Prepare model for XAI\n",
    "model = generate_XAI_model(model=model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ee9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (image_batch, mask_batch, paths) in tqdm(enumerate(test_loader)):\n",
    "    image_batch = image_batch.to(device, dtype=torch.float)\n",
    "    image_name = paths[0][0].split(\"/\")[-1].split(\".\")[0]\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73cf62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,2,figsize=(8,6))\n",
    "_ = visualize_image_and_mask(image_batch.squeeze().detach().cpu(),mask_batch.detach().cpu(), dataset_type=dataset_type,fig=fig,axes=axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f78216a",
   "metadata": {},
   "source": [
    "### Attribution Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose method between LayerLRP, LayerDeepLift, LayerGradientXActivation, LayerGradCam\n",
    "method = \"LayerLRP\"\n",
    "selected_target = 1 if dataset_type == \"UTP\" else 1\n",
    "\n",
    "layers = LUNET_LAYERS_JOURNAL if model_type == \"lunet\" else UNET_LAYERS_JOURNAL\n",
    "layers = list(layers.keys())\n",
    "layer_name = layers[-2];print(layer_name)\n",
    "\n",
    "layer_mapper = {\n",
    "    \"conv1.0\": \"Dec4\",\n",
    "    \"conv2.0\": \"Dec3\",\n",
    "    \"conv3.0\": \"Dec2\",\n",
    "    \"conv4.0\": \"Dec1\",    \n",
    "    \"conv5.0\": \"Dec4\",\n",
    "    \"conv6.0\": \"Dec3\",\n",
    "    \"conv7.0\": \"Dec2\",\n",
    "    \"conv8.0\": \"Dec1\",\n",
    "    \"final_layer\": \"FL\",\n",
    "}\n",
    "\n",
    "attr = generateAttributions(image_batch, model, selected_target, method, get_layer_by_name(model, layer_name))\n",
    "attr = clip_fixed_percentage(attr,p=0.05)\n",
    "\n",
    "if attr.max() > 10e4:\n",
    "    attr = Normalizations.pick('normalize_log')(attr)\n",
    "\n",
    "pv,nv,zv = split_components(attr,zero_threshold=0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989b7e3a",
   "metadata": {},
   "source": [
    "### Visualize heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,3,figsize=(10,6))\n",
    "fig.suptitle(f\"{method} on {layer_mapper[layer_name]}\",y=0.95)\n",
    "\n",
    "saliency_mask = pv - nv\n",
    "if nv.sum() == 0:\n",
    "    cmap=\"YlGn\"\n",
    "elif pv.sum() == 0:\n",
    "    cmap=\"YlOrRd\"\n",
    "    saliency_mask = -saliency_mask\n",
    "else:\n",
    "    cmap=\"RdYlGn\"\n",
    "_ = visualize_image(image_batch.squeeze().detach().cpu(),fig=fig,ax=axes[0]);axes[0].set_title(\"Saliency Overlay\")\n",
    "axes[0].imshow(pv.detach().cpu().squeeze().numpy(), cmap=cmap, alpha=0.5); axes[0].axis(\"off\")\n",
    "            \n",
    "_ = generate_heatmap(pv, fig=fig, ax=axes[1],title=\"Positive\")\n",
    "_ = generate_heatmap(-nv, fig=fig, ax=axes[2],title=\"Negative\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camera",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
