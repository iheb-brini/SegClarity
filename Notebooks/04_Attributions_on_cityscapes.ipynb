{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a7ee11",
   "metadata": {},
   "source": [
    "## Notebook Title: Attributions on Cityscapes\n",
    "\n",
    "This notebook demonstrates how SegClarity can be used to compute, visualize, and interpret attributions for deep learning models trained on the Cityscapes semantic segmentation dataset. It leverages XAI techniques to better understand model predictions at the pixel and region level for urban scene understanding.\n",
    "#\n",
    "**Purpose**:  \n",
    "- To apply attribution methods (e.g., Grad-CAM, DeepLift, LRP) to segmentation models trained on Cityscapes images.\n",
    "- To visualize and compare attribution maps with original street scene images and ground truth masks.\n",
    "- To facilitate model interpretability and support error analysis in semantic segmentation for urban environments.\n",
    "#\n",
    "**Key Features**:\n",
    "- Loads pre-trained segmentation model (UNET) for the Cityscapes dataset.\n",
    "- Supports multiple attribution methods from Captum and custom utilities.\n",
    "- Generates and visualizes attribution heatmaps over Cityscapes images.\n",
    "- Provides side-by-side comparison of attributions, input images, and segmentation masks.\n",
    "#\n",
    "**Workflow**:\n",
    "1. Configure dataset and model parameters for Cityscapes.\n",
    "2. Load the appropriate pre-trained segmentation model.\n",
    "3. Select and apply attribution methods to validation images.\n",
    "4. Visualize attribution maps alongside original images and ground truth.\n",
    "5. Analyze and interpret the results for model transparency and insight into model behavior.\n",
    "#\n",
    "**Dataset**:  \n",
    "- Cityscapes: 20-class semantic segmentation of urban street scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8de6db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Modules.Architecture import generate_model\n",
    "from Modules.ModelXAI import generate_XAI_model\n",
    "\n",
    "from Modules.Utils import (\n",
    "    get_layer_by_name,\n",
    "    clip_fixed_percentage,\n",
    "    Normalizations,\n",
    ")\n",
    "\n",
    "from Modules.Attribution.constants import (\n",
    "    LUNET_LAYERS_JOURNAL,\n",
    "    UNET_LAYERS_JOURNAL,\n",
    ")\n",
    "from Modules.Attribution import generateAttributions\n",
    "\n",
    "from Modules.Visualization import (\n",
    "    generate_heatmap,\n",
    ")\n",
    "from Modules.Visualization.tools import split_components\n",
    "from Modules.Visualization.core import visualize_image\n",
    "\n",
    "from Modules.CityscapeDataset.core import create_cityscapes_dataloaders\n",
    "from Modules.CityscapeDataset.tools import decode_segmap\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "absolute_path = Path('..')\n",
    "\n",
    "dataset_type='cityscapes'\n",
    "\n",
    "_, val_loader, _  = create_cityscapes_dataloaders(\n",
    "    root=absolute_path / \"datasets\" / \"cityscapes\",\n",
    "    batch_size=1,\n",
    "    num_workers=2,\n",
    "    augment_train=None,\n",
    ")\n",
    "\n",
    "\n",
    "base_dir: str = f\"../datasets/{dataset_type}\"\n",
    "device = 'cuda:0'\n",
    "model_type = 'unet'\n",
    "\n",
    "OUT_CHANNELS = 20\n",
    "models_path = f\"../models/{dataset_type}/{model_type}\"\n",
    "model_name = f\"best_model.pth\"\n",
    "\n",
    "\n",
    "print(f\"Loading model from {models_path}\")\n",
    "print(f\"Model name: {model_name}\")\n",
    "\n",
    "\n",
    "# %%\n",
    "print(\"Loading model...\")\n",
    "model = generate_model(\n",
    "    model_type= model_type, \n",
    "    out_channels=OUT_CHANNELS, \n",
    "    load_from_checkpoint=True,\n",
    "    models_path=models_path, \n",
    "    checkpoint_name=model_name\n",
    ").eval().to(device)\n",
    "\n",
    "# Prepare model for XAI\n",
    "model = generate_XAI_model(model=model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ee9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch,mask_batch,names in val_loader:\n",
    "    image_batch = image_batch.to(device)\n",
    "    mask_batch = mask_batch.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73cf62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred= model(image_batch).max(dim=1)[1]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(image_batch[0].permute(1, 2, 0).detach().cpu().numpy() * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]));_ = ax[0].axis('off'); ax[0].set_title('Image')\n",
    "ax[1].imshow(decode_segmap(mask_batch[0].detach().cpu()));_ = ax[1].axis('off'); ax[1].set_title('Mask')\n",
    "fig.tight_layout()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f78216a",
   "metadata": {},
   "source": [
    "### Attribution Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose method between LayerLRP, LayerDeepLift, LayerGradientXActivation, LayerGradCam\n",
    "method = \"LayerDeepLift\"\n",
    "selected_target = 14\n",
    "\n",
    "layers = LUNET_LAYERS_JOURNAL if model_type == \"lunet\" else UNET_LAYERS_JOURNAL\n",
    "layers = list(layers.keys())\n",
    "layer_name = layers[-3];print(layer_name)\n",
    "\n",
    "layer_mapper = {\n",
    "    \"conv1.0\": \"Dec4\",\n",
    "    \"conv2.0\": \"Dec3\",\n",
    "    \"conv3.0\": \"Dec2\",\n",
    "    \"conv4.0\": \"Dec1\",    \n",
    "    \"conv5.0\": \"Dec4\",\n",
    "    \"conv6.0\": \"Dec3\",\n",
    "    \"conv7.0\": \"Dec2\",\n",
    "    \"conv8.0\": \"Dec1\",\n",
    "    \"final_layer\": \"FL\",\n",
    "}\n",
    "\n",
    "attr = generateAttributions(image_batch, model, selected_target, method, get_layer_by_name(model, layer_name))\n",
    "attr = clip_fixed_percentage(attr,p=0.05)\n",
    "\n",
    "if attr.max() > 10e4:\n",
    "    attr = Normalizations.pick('normalize_log')(attr)\n",
    "\n",
    "pv,nv,zv = split_components(attr,zero_threshold=0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989b7e3a",
   "metadata": {},
   "source": [
    "### Visualize heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,3,figsize=(16,8))\n",
    "fig.suptitle(f\"{method} on {layer_mapper[layer_name]}\",y=0.75)\n",
    "\n",
    "\n",
    "saliency_mask = pv - nv\n",
    "if nv.sum() == 0:\n",
    "    cmap=\"YlGn\"\n",
    "elif pv.sum() == 0:\n",
    "    cmap=\"YlOrRd\"\n",
    "    saliency_mask = -saliency_mask\n",
    "else:\n",
    "    cmap=\"RdYlGn\"\n",
    "viz_image = image_batch.squeeze().detach().cpu()\n",
    "\n",
    "viz_image = torch.from_numpy(resize(viz_image, (3,*attr.shape[-2:]), anti_aliasing=True))\n",
    "\n",
    "print(viz_image.shape)\n",
    "\n",
    "_ = visualize_image(viz_image,fig=fig,ax=axes[0]);axes[0].set_title(\"Saliency Overlay\")\n",
    "axes[0].imshow(pv.detach().cpu().squeeze().numpy(), cmap=cmap, alpha=0.5); axes[0].axis(\"off\")\n",
    "            \n",
    "_ = generate_heatmap(pv, fig=fig, ax=axes[1],title=\"Positive\")\n",
    "_ = generate_heatmap(nv, fig=fig, ax=axes[2],title=\"Negative\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camera",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
